# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FG3LrVgMTVFnUh5jBU4EOcp_x6Zr8YDT
"""

import csv
import requests
import os.path
from os import path
def readCsvFile():
    with open('dmoztools_URLs.csv') as csvfile:
        readCSV = csv.reader(csvfile, delimiter=',')
        readURLs = []
        for row in readCSV:
            url = row[1]
            lastID = row[0]
            readURLs.append(url)
    csvfile.close()
    return readURLs,lastID
def createCsvFile(URLs):
    csv.register_dialect('myDialect',
    quoting=csv.QUOTE_ALL,
    skipinitialspace=True)
    with open('benign_URLs.csv', 'w') as f:
        writer = csv.writer(f, dialect='myDialect')
        for row in URLs:
            writer.writerow(row)
    f.close()
def appendRowsToCsvFile(URLs):
    with open('benign_URLs.csv', 'a') as csvFile:
        writer = csv.writer(csvFile)
        for row in URLs:
            writer.writerow(row)
    csvFile.close()

if(path.exists("dmoztools_URLs.csv")):
    count=0
    url,lastID = readCsvFile()
    url = url[0:55000]                              #Csv dosyasındaki ilk 55000 urli düzetmek istediğimizi belirttim.Ama bu kod çok yavaş.55000 url bir
    fixed_url = []                                  #arada bu kodla düzeltilemez.Biz 10000 er alıp düzelttik.O da yavaş sürdü gerçi.
    for i in range(len(url)):
        try:
            r = requests.get(url[i])                 #Urlleri requestsle gonderme
            fixed_url.append(r.url)                  #Urllerin düzeltilmiş halini alma
            count=count+1
            print(str(count)+","+r.url)
        except:
            continue                                #Kod çalıştıkça kaçıncı urli çektiğini print ile yazdırıyouz. Bazen duraksamalar yaşanıyor.
    urlSize = len(fixed_url)                        #Duraksama olduğunda colab deki çalışma-durdurma bölümü olan yere tıklıyorum. Interrupt execution 
    URLs = []                                       #yazıyor çalışma esnasında. Ona tıklayınca çalışmaya devam ediyor. 
    for i in range(urlSize):                        #Bu kodda tüm urlleri çektikten sonra csv dosyasına yazdırıyor. Yani durdurunca çekitiği kadarını 
        row = []                                    #yazdırmıyor. Biz çok vakit kaybetmemek için uğraşamadık.Siz ilgilenirseniz daha iyi bir kod 
        for j in range(3):                          #yazabilirsiniz hocam. Zaten bu kodda işinize yarayabilecek iki satır kod var.
            if (j % 3 == 0):                        #r=requests.get(url[i]) ve fixed_url.append(r.url)  Gerisini kendi ihtiyacınıza göre düzenlersiniz.
                row.append(i + 1)
            elif (j % 3 == 1):
                row.append(fixed_url[i])
            elif (j % 3 == 2):
                row.append(1)
        URLs.append(row)
    if(path.exists("benign_URLs.csv")):
        appendRowsToCsvFile(URLs)
    else:
        createCsvFile(URLs)
else:
    print("There is no file named 'dmoztools_URLs.csv'")