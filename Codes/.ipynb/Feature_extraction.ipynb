{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP8h3koSAD62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -q whois\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import math\n",
        "#import whois\n",
        "import os.path\n",
        "from os import path\n",
        "def readCsvFile():\n",
        "    with open('URLs.csv') as csvfile:\n",
        "        readCSV = csv.reader(csvfile, delimiter=',')\n",
        "        readURLs = []\n",
        "        kinds = []\n",
        "        for row in readCSV:\n",
        "            url = row[1]\n",
        "            kind = row[2]\n",
        "            readURLs.append(url)\n",
        "            kinds.append(kind)\n",
        "    csvfile.close()\n",
        "    return readURLs,kinds\n",
        "def createCsvFile(URLs):\n",
        "    csv.register_dialect('myDialect',\n",
        "    quoting=csv.QUOTE_ALL,\n",
        "    skipinitialspace=True)\n",
        "    with open('URLs.csv', 'w') as f:\n",
        "        writer = csv.writer(f, dialect='myDialect')\n",
        "        writer.writerow([\"id\",\"url\",\"type\",\"ip\",\"https\",\"spam\",\"#.\",\"#/\",\"#numbers\",\"sensitive_words\",\"uppercase_letter\",\"length\",\"suspicious_character\",\n",
        "                         \"prefix-suffix\",\"tld\",\"entropy\"])\n",
        "        for row in URLs:\n",
        "            writer.writerow(row)\n",
        "    f.close()\n",
        "def getTLDList():\n",
        "    r = requests.get(\"http://data.iana.org/TLD/tlds-alpha-by-domain.txt\")\n",
        "    soup = BeautifulSoup(r.content)\n",
        "    tldList = []\n",
        "    d = soup.findAll(text=True)\n",
        "    for line in d:\n",
        "        for line2 in line.strip().split('\\n'):\n",
        "            if line2:\n",
        "                tldList.append(line2)\n",
        "    tldList = tldList[1:]\n",
        "    return tldList\n",
        "def is_ip(url):\n",
        "    ip = False\n",
        "    length = len(url)\n",
        "    for i in range(length):\n",
        "        count = 0\n",
        "        j = i\n",
        "        while(j<length and ((ord(url[j])>=48 and ord(url[j])<=57) or ord(url[j])==46)):\n",
        "            if(ord(url[j])==46):             #Ascii of the dot\n",
        "                count = count + 1\n",
        "            j = j + 1\n",
        "        if(count==3):\n",
        "            ip = True\n",
        "    if(ip == True):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def is_https(url):\n",
        "    if(url[3] == 'p' and url[4] == 's'):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "def is_spam(url):\n",
        "    u = url[:4]\n",
        "    if(u == \"SPAM\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def numberOfDots(url):\n",
        "    if(url[8]=='w'):\n",
        "        if(url[4]=='s'):\n",
        "            i = 12\n",
        "        else:\n",
        "            i = 11\n",
        "    else:\n",
        "        if(url[4]=='s'):\n",
        "            i = 8\n",
        "        else:\n",
        "            i = 7\n",
        "    count = 0\n",
        "    length = len(url)\n",
        "    while(i<length):\n",
        "        if(url[i]=='.'):\n",
        "            count = count + 1\n",
        "        i = i + 1\n",
        "    if(count>2):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def numberOfSlashes(url):\n",
        "    length = len(url)\n",
        "    count = 0\n",
        "    for i in range(length):\n",
        "        if(url[i] == '/'):\n",
        "            count = count + 1\n",
        "    if(count>6):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def numberOfNumbers(url):\n",
        "    length = len(url)\n",
        "    count = 0\n",
        "    for i in range(length):\n",
        "        if(ord(url[i])>=48 and ord(url[i])<=57):\n",
        "            count = count + 1\n",
        "    if(count>6):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def existenceOfSensitiveWords(url):\n",
        "    phishingWords = [\"secure\",\"account\",\"login\",\"signin\",\"confirm\",\"submit\"]\n",
        "    phishing = False\n",
        "    for i in range(len(phishingWords)):\n",
        "        if(phishingWords[i] in url):\n",
        "            phishing = True\n",
        "    if(phishing == True):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def existenceOfUpperCaseLetter(url):\n",
        "    length = len(url)\n",
        "    phishing = False\n",
        "    for i in range(length):\n",
        "        if(ord(url[i])>=65 and ord(url[i])<=90):\n",
        "            phishing = True\n",
        "    if(phishing == True):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def length(url):\n",
        "    if(len(url) > 100):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def has_suspicious_char(url):\n",
        "    if(url.find(\"@\") != -1 or url.find(\"&\") != -1 or url.count(\"//\")>1):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def prefix_suffix(url):\n",
        "    if(url.find(\"-\") != -1):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def TLD_count(url,tldList):\n",
        "    tld_Found = False\n",
        "    slash_count = 0\n",
        "    if(url.count('/') == 2):\n",
        "        url = url + \"/\"\n",
        "    i = 0\n",
        "    stop = 0\n",
        "    while(i<len(url) and stop==0):\n",
        "        if(url[i] == '/'):\n",
        "            slash_count = slash_count + 1\n",
        "        if(slash_count==3):\n",
        "            stop = 1\n",
        "        i = i + 1\n",
        "    u = url[i:]\n",
        "    for tld in tldList:\n",
        "        if(u.find(\".\"+tld.lower()+\".\") != -1 or\n",
        "           u.find(\".\"+tld.lower()+\"/\") != -1 or\n",
        "           u.find(\".\"+tld.lower()+\"&\") != -1):\n",
        "            tld_Found = True\n",
        "    if(tld_Found == True):\n",
        "        return 0\n",
        "    else: \n",
        "        return 1 \n",
        "def entropy(url):\n",
        "    if not url:\n",
        "        return 0\n",
        "    entropy = 0\n",
        "    for x in range(256):\n",
        "        p_x = float(url.count(chr(x)))/len(url)\n",
        "        if(p_x > 0):\n",
        "            entropy += - p_x*math.log(p_x, 2)\n",
        "    if(entropy>4.8):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "def domain_age(url):\n",
        "    res = whois.whois(url)\n",
        "    try:\n",
        "        print(res.creation_date.year)\n",
        "        if(res.creation_date.year>2013):\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        print(\"An exception occurred\")\n",
        "        return 0\n",
        "def alexarank(url):\n",
        "    r = requests.get(\"https://www.alexa.com/siteinfo/\"+url)\n",
        "    soup = BeautifulSoup(r.content)\n",
        "    links = soup.find_all(\"div\")\n",
        "    i = 0\n",
        "    stop = 0\n",
        "    while(i<len(links) and stop==0):\n",
        "        if(\"This site ranks:\" == links[i].text):\n",
        "            rank = links[i+4].text\n",
        "            rank = rank.replace(',','')\n",
        "            rank = rank[4:10]\n",
        "            stop = 1\n",
        "        i = i + 1\n",
        "    try:\n",
        "        if(int(rank)<150000):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "url,kind = readCsvFile()\n",
        "ip = []\n",
        "https = []\n",
        "spam = []\n",
        "number_of_dots = []\n",
        "number_of_slashes = []\n",
        "number_of_numbers = []\n",
        "sensitive_words = []\n",
        "upper_case_letter = []\n",
        "length_ = []\n",
        "suspicious_character = []\n",
        "prefixSuffix = []\n",
        "tld = []\n",
        "entropy_ = []\n",
        "tldList = getTLDList()\n",
        "for i in range(len(url)):\n",
        "    ip.append(is_ip(url[i]))\n",
        "    https.append(is_https(url[i]))\n",
        "    spam.append(is_spam(url[i]))\n",
        "    number_of_dots.append(numberOfDots(url[i]))\n",
        "    number_of_slashes.append(numberOfSlashes(url[i]))\n",
        "    number_of_numbers.append(numberOfNumbers(url[i]))\n",
        "    sensitive_words.append(existenceOfSensitiveWords(url[i]))\n",
        "    upper_case_letter.append(existenceOfUpperCaseLetter(url[i]))\n",
        "    length_.append(length(url[i]))\n",
        "    suspicious_character.append(has_suspicious_char(url[i]))\n",
        "    prefixSuffix.append(prefix_suffix(url[i]))\n",
        "    tld.append(TLD_count(url[i],tldList))\n",
        "    entropy_.append(entropy(url[i]))\n",
        "URLs = []\n",
        "for i in range(len(url)):\n",
        "    row = []\n",
        "    for j in range(16):\n",
        "        if (j % 16 == 0):\n",
        "            row.append(i + 1)\n",
        "        elif (j % 16 == 1):\n",
        "            row.append(url[i])\n",
        "        elif (j % 16 == 2):\n",
        "            row.append(kind[i])\n",
        "        elif (j % 16 == 3):\n",
        "            row.append(ip[i])            \n",
        "        elif (j % 16 == 4):\n",
        "            row.append(https[i])\n",
        "        elif (j % 16 == 5):\n",
        "            row.append(spam[i])\n",
        "        elif (j % 16 == 6):\n",
        "            row.append(number_of_dots[i])    \n",
        "        elif (j % 16 == 7):\n",
        "            row.append(number_of_slashes[i])\n",
        "        elif (j % 16 == 8):\n",
        "            row.append(number_of_numbers[i])\n",
        "        elif (j % 16 == 9):\n",
        "            row.append(sensitive_words[i])\n",
        "        elif (j % 16 == 10):\n",
        "            row.append(upper_case_letter[i])\n",
        "        elif (j % 16 == 11):\n",
        "            row.append(length_[i])  \n",
        "        elif (j % 16 == 12):\n",
        "            row.append(suspicious_character[i])\n",
        "        elif (j % 16 == 13):\n",
        "            row.append(prefixSuffix[i])\n",
        "        elif (j % 16 == 14):\n",
        "            row.append(tld[i])\n",
        "        elif (j % 16 == 15):\n",
        "            row.append(entropy_[i])     \n",
        "    URLs.append(row)\n",
        "createCsvFile(URLs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}