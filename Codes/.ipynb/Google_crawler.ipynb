{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google_crawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT1xd-d7p2eI",
        "colab_type": "code",
        "outputId": "cf1d9bfc-5e2d-44e1-b0a5-d4d2a6abfe1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -q fake_useragent\n",
        "import csv\n",
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib\n",
        "import re\n",
        "from os import path\n",
        "def crawlFromGoogle(readURLs,id):\n",
        "    queryWords = [\"banks\", \"amazon\", \"yahoo finance\", \"stock market\", \"financas\", \"stock\", \"efinance\", \"finance\", \"loans\", \"google market\", \"stripe\", \"dwolla\",\n",
        "                  \"apple pay\", \"payoneer\", \"2checkout\", \"square\", \"payza\", \"pay\", \"skrill\", \"venmo\", \"google wallet\", \"wepay\", \"cryptocurrencies\", \"cash\", \n",
        "                  \"e-wallets\", \"credit card\", \"paypal\", \"adyen\", \"paymill\", \"due\", \"fedex\", \"bitcoin\", \"mbill\", \"online store\", \"online business\", \"shopping\",\n",
        "                  \"online payment\" ,\"zendesk\", \"docusign\", \"dropbox\", \"slack\", \"concur\", \"amazon web services\", \"salesforce\", \"sale\", \"discount\", \"raise\", \n",
        "                  \"e-commerce\", \"saas\", \"finance\", \"money\"]\n",
        "    url = []\n",
        "    for i in queryWords:\n",
        "        query = i\n",
        "        query = urllib.parse.quote_plus(query) # Format into URL encoding\n",
        "        number_result = 100\n",
        "        ua = UserAgent()\n",
        "        google_url = \"https://www.google.com/search?q=\" + query + \"&num=\" + str(number_result)\n",
        "        response = requests.get(google_url, {\"User-Agent\": ua.random})\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        result_div = soup.find_all('div', attrs = {'class': 'ZINbbc'})\n",
        "        links = []\n",
        "        titles = []\n",
        "        descriptions = []\n",
        "        for r in result_div:\n",
        "            # Checks if each element is present, else, raise exception\n",
        "            try:\n",
        "                link = r.find('a', href = True)\n",
        "                title = r.find('div', attrs={'class':'vvjwJb'}).get_text()\n",
        "                description = r.find('div', attrs={'class':'s3v9rd'}).get_text() \n",
        "                # Check to make sure everything is present before appending\n",
        "                if link != '' and title != '' and description != '': \n",
        "                    links.append(link['href'])\n",
        "                    titles.append(title)\n",
        "                    descriptions.append(description)\n",
        "            # Next loop if one element is not present\n",
        "            except:\n",
        "                continue\n",
        "        to_remove = []\n",
        "        clean_links = []\n",
        "        for j, l in enumerate(links):\n",
        "            clean = re.search('\\/url\\?q\\=(.*)\\&sa',l)\n",
        "            # Anything that doesn't fit the above pattern will be removed\n",
        "            if clean is None:\n",
        "                to_remove.append(j)\n",
        "                continue\n",
        "            clean_links.append(clean.group(1))\n",
        "        url = list(set(url + clean_links))\n",
        "    url = list(set(url) - set(readURLs))\n",
        "    url = list(dict.fromkeys(url))  # removing duplicates\n",
        "    querySize = len(url)\n",
        "    URLs = []\n",
        "    for i in range(querySize):\n",
        "        row = []\n",
        "        for j in range(3):\n",
        "            if (j % 3 == 0):\n",
        "                row.append(id + 1)\n",
        "            elif (j % 3 == 1):\n",
        "                row.append(url[i])\n",
        "            elif (j % 3 == 2):\n",
        "                row.append(1)\n",
        "        URLs.append(row)\n",
        "        id = id + 1\n",
        "    return URLs\n",
        "def createCsvFile(URLs):\n",
        "    csv.register_dialect('myDialect',\n",
        "    quoting=csv.QUOTE_ALL,\n",
        "    skipinitialspace=True)\n",
        "    with open('benign_URLs.csv', 'w') as f:\n",
        "        writer = csv.writer(f, dialect='myDialect')\n",
        "        for row in URLs:\n",
        "            writer.writerow(row)\n",
        "    f.close()\n",
        "def appendRowsToCsvFile(URLs):\n",
        "    with open('benign_URLs.csv', 'a') as csvFile:\n",
        "        writer = csv.writer(csvFile)\n",
        "        for row in URLs:\n",
        "            writer.writerow(row)\n",
        "    csvFile.close()\n",
        "def readCsvFile():\n",
        "    with open('benign_URLs.csv') as csvfile:\n",
        "        readCSV = csv.reader(csvfile, delimiter=',')\n",
        "        readURLs = []\n",
        "        for row in readCSV:\n",
        "            url = row[1]\n",
        "            lastID = row[0]\n",
        "            readURLs.append(url)\n",
        "    csvfile.close()\n",
        "    return readURLs,lastID\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "if(path.exists(\"benign_URLs.csv\")):\n",
        "    readURLs,lastID = readCsvFile()\n",
        "    URLs = crawlFromGoogle(readURLs,int(lastID))\n",
        "    appendRowsToCsvFile(URLs)    \n",
        "else:\n",
        "    readURLs = []\n",
        "    URLs = crawlFromGoogle(readURLs,0)\n",
        "    createCsvFile(URLs)        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}